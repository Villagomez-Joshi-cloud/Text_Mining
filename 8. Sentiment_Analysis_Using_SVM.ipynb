{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Aito6vUNvJoO"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#Tokenization of text\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "#remove stop-words\n",
    "from nltk.corpus import stopwords # library \n",
    "nltk.download('stopwords')\n",
    "all_stopwords = set(stopwords.words('english')) # set the language \n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lFwT10BkvY_I"
   },
   "outputs": [],
   "source": [
    "# reading review data with panda frames \n",
    "reviews_data=pd.read_csv('IMDB Dataset.csv')\n",
    "reviews_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pTItnRutvdY-"
   },
   "outputs": [],
   "source": [
    "#sentiment counts\n",
    "reviews_data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eVaIirz2vgeo"
   },
   "outputs": [],
   "source": [
    "# The sentiments are either 'positive' or 'negative' and are evenly distributed. Lets preprocess the text using the simple tokenizer we built in last class. We call it preprocess_text now.\n",
    "def preprocess_text(text: str) -> List[str]:\n",
    "    # Looking at the text we see that <br></br> which is HTML tag for line break can be a good splitter\n",
    "    # A sentence (atleast well structured) often has a full spot at the end. We use these two for word breaks\n",
    "    pattern1 = re.compile(\"<br /><br />|\\.\")\n",
    "    lines = re.split(pattern1, text)\n",
    "    # you can break a sentence into words using whitespace based split\n",
    "    tokens = []\n",
    "    for line in lines:\n",
    "        tokens += line.split(\" \")\n",
    "\n",
    "    # lowercase and remove any non-alphanumeric characters from tokens for normalize\n",
    "    normalized_tokens = [re.sub(r\"\\W+\", \"\", token.lower()) for token in tokens]\n",
    "    return  \" \".join([\n",
    "            token\n",
    "            for token in normalized_tokens\n",
    "            if token and token not in all_stopwords and len(tokens) > 1 \n",
    "        ])\n",
    "    \n",
    "\n",
    "  \n",
    "custom_review = \"I hated the film. It was a disaster. Poor direction, bad acting.\"\n",
    "custom_review_tokens = preprocess_text(custom_review)\n",
    "print(custom_review_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ixHaodMrvjMz"
   },
   "outputs": [],
   "source": [
    "#apply preprocessing to review data\n",
    "reviews_data['review'] = reviews_data['review'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9epGclp8vl4Z"
   },
   "outputs": [],
   "source": [
    "#split the dataset  \n",
    "#train dataset\n",
    "train_reviews=reviews_data.review[:40000]\n",
    "train_sentiments=reviews_data.sentiment[:40000]\n",
    "#test dataset\n",
    "test_reviews=reviews_data.review[40000:45000]\n",
    "test_sentiments=reviews_data.sentiment[40000:45000]\n",
    "#validation (blind) dataset\n",
    "blind_reviews=reviews_data.review[45000:]\n",
    "blind_sentiments=reviews_data.sentiment[45000:]\n",
    "print(train_reviews.shape,train_sentiments.shape)\n",
    "print(test_reviews.shape,test_sentiments.shape)\n",
    "print(blind_reviews.shape,blind_sentiments.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "owwrkzpGvo7F"
   },
   "outputs": [],
   "source": [
    "# CountVectorizer implements both tokenization and occurrence counting in a single class. Read more here https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "# You can also reuse the from scratch code we learnt in previous class\n",
    "# TfidfVectorizer Convert a collection of raw documents to a matrix of TF-IDF features. Equivalent to CountVectorizer followed by TfidfTransformer.\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#Count vectorizer with \n",
    "lower_count_thr = 100 # rare words/tokens\n",
    "upper_count_thr = 5000 # frequent/common tokens\n",
    "\n",
    "tv=TfidfVectorizer(min_df=lower_count_thr,max_df=upper_count_thr,binary=False,ngram_range=(1,1))\n",
    "#transformed train reviews\n",
    "tv_train_reviews=tv.fit_transform(train_reviews)\n",
    "#transformed test reviews\n",
    "tv_test_reviews=tv.transform(test_reviews)\n",
    "\n",
    "#transformed validation reviews\n",
    "tv_blind_reviews=tv.transform(blind_reviews)\n",
    "\n",
    "print('BOW_cv_train:',tv_train_reviews.shape)\n",
    "print('BOW_cv_test:',tv_test_reviews.shape)\n",
    "print('BOW_cv_blind:',tv_blind_reviews.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dkpW3QbwvrgH"
   },
   "outputs": [],
   "source": [
    "#Now generate binary (true, false) labels from sentiment values. positive maps to 1, negative maps to 0\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lb=LabelBinarizer()\n",
    "#transformed sentiment data\n",
    "sentiment_data=lb.fit_transform(reviews_data['sentiment'])\n",
    "print(sentiment_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ihFhF2mDvt5Z"
   },
   "outputs": [],
   "source": [
    "#Spliting the sentiment data\n",
    "train_sentiments=sentiment_data[:40000]\n",
    "test_sentiments=sentiment_data[40000:45000]\n",
    "blind_sentiments=sentiment_data[45000:]\n",
    "print(train_sentiments.shape)\n",
    "print(test_sentiments.shape)\n",
    "print(blind_sentiments.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RXGRPFMrvw2R"
   },
   "outputs": [],
   "source": [
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(tv_train_reviews,train_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4cIQwAP7vzzo"
   },
   "outputs": [],
   "source": [
    "# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(tv_test_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hfBbsqeBv2ba"
   },
   "outputs": [],
   "source": [
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, test_sentiments)*100)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Sentiment Analysis Using SVM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
